Code:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=1000, centers=2,
                  random_state=0, cluster_std=1.3)
plt.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='bwr')
plt.colorbar()
plt.xlabel('x1')
plt.ylabel('x2')
plt.show()
def euclidean_distance(row1, row2):
    distance = 0.0
    for i in range(len(row1)-1):
        distance +=  (row1[i] - row2[i])**2
        return (distance)**0.5
distance = euclidean_distance(X[0], X)
def get_neighbors(train, test_row, num_neighbors):
 distances = list()
 for train_row in train:
    dist = euclidean_distance(test_row, train_row)
    distances.append((train_row, dist))
    distances.sort(key=lambda tup: tup[1])
    neighbors = list()
    #print(distances)
 for i in range(num_neighbors):
    neighbors.append(distances[i][0])
 return neighbors
neighbors = get_neighbors(X, X[0], 3)
neighbors
ef predict_classification(train, test_row, num_neighbors):
    neighbors = get_neighbors(train, test_row, num_neighbors)
    output_values = [row[-1] for row in neighbors]
    prediction = max(set(output_values), key=output_values.count)
    return prediction

# Adding a column to the array using concatenate()
Z=np.concatenate([X, y.reshape(-1,1)], axis=1)
prediction = predict_classification(Z, Z[0], 3)
print('Expected %d, Got %d.' % (y[0], prediction))
from sklearn.model_selection import train_test_split
# Split the dataset into the training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=0) # 70% training and 30% test
from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=3)

# Train the model using the training sets
model.fit(X_train,y_train)
from sklearn import metrics
#Predict Output
predicted= model.predict(X_test) 
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, predicted))
from mlxtend.plotting import plot_decision_regions

# Plotting decision regions
plot_decision_regions(X ,y, clf=model, legend=2)

plt.title('K=3')
plt.show()

Q1. What is accuracy of training and testing data?

a) The accuracy of the train set is 0.93

b) The accuracy of the train set is 0.78

c) The accuracy of the test set is 0.90

d) The accuracy of the test set is 0.80

Correct answer: a) and c)

Q2. k-NN algorithm does more computation on test time rather than train time.

a) True

b) False

Correct answer: a)

Q3. Which of the following option is true about KNN algorithm?

a) It can be used for classification

b) It can be used in both classification and regression

c) It can be used for regression

Correct answer: b)

Q4. Which of the following statement is true about KNN algorithm ?
a) k-NN makes assumptions about the functional form of the problem being solved.
b) KNN performs much better if all of the data have the same scale.
c) KNN works well with a small number of input variables.

Correct answer: b) and c)

Q5. The Figure below illustrates decision boundaries for two nearest-neighbour classifiers. Determine which one of the boundaries belongs to the 1-nearest neighbour classifier.

a) Decision boundary 2

b) Decision boundary 1

Correct answer: a)


